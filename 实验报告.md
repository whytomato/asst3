# 实验报告

> 学号：21373224
>
> 姓名：王浩俨

本次实验选择的题目是[Assignment 3: A Simple CUDA Renderer](https://github.com/stanford-cs149/asst3)，这是完成题目的[仓库](https://github.com/whytomato/asst3)。

在实验中，我选择使用`wsl`进行实验的运行与评测，GPU为电脑的`NVIDIA GeForce RTX 3070 Ti`。

[TOC]

## 一、题目要求

**截止日期:11月8日星期三太平洋标准时间晚上11:59**

**总分100分**

![My Image](handout/teaser.jpg?raw=true)

### 概述

在本作业中，您将在CUDA中编写一个绘制彩色圆圈的并行渲染器。虽然这个呈现器非常简单，但是并行化呈现器需要您设计和实现数据结构可以有效地并行构造和操作。这是一个具有挑战性的任务，所以建议你早点开始。**说真的，建议您尽早开始。**好运！

### 环境配置

1. 您将在亚马逊网络服务（AWS）上支持 GPU 的虚拟机上收集本作业的结果（即运行性能测试）。请按照 [cloud_readme.md](cloud_readme.md) 中的说明设置机器以运行作业。

2. 从课程 Github 下载作业启动代码，请使用

   `git clone https://github.com/stanford-cs149/asst3`

   CUDA C 程序员指南 PDF 版或网络版是学习如何使用 CUDA 编程的绝佳参考。网上（只需谷歌！）和英伟达开发者网站上有大量的 CUDA 教程和 SDK 示例。您可能会特别喜欢免费的 Udacity 课程《CUDA 并行编程入门》。

   《CUDA C 语言编程指南》中的表 G.1 是一个方便的参考资料，其中列出了本作业中使用的英伟达 T4 GPU 每个线程块的最大 CUDA 线程数、线程块大小、共享内存等。英伟达 T4 GPU 支持 CUDA 7.5 计算能力。

   对于 C++ 问题（如 virtual 关键字是什么意思），C++ Super-FAQ 是一个很好的资源，它的解释详细而易懂（与很多 C++ 资源不同），由 C++ 的创建者 Bjarne Stroustrup 共同编写！

#### 警告

为节省资源，虚拟机将在 CPU 使用率低于 2% 的 15 分钟后自动停止。

这意味着，如果您没有进行诸如编写代码之类的 CPU 密集型工作，虚拟机就会关闭。

因此，我们建议您在本地开发代码，然后手动将代码复制到虚拟机上，或者使用 git 将提交的代码拉到虚拟机上。使用 git 的好处在于，您可以回溯到之前的代码版本。

如果您之前没有建立过私有的 git 仓库，这里有一些资源可以帮助您入门。请确保 github 仓库是私有的，以保证您不会违反荣誉守则。

设置 git 链接：

- 添加远程仓库，连接到你的私有仓库。
- 添加 ssh 密钥以设置 ssh 密钥。我们建议不设密码，使用默认名称 id_rsa。

有了 ssh 密钥并知道如何连接远程版本库后，你需要做以下两件事来设置你的环境。

1. 将你的私钥复制到服务器的 .ssh 文件夹（id_rsa 在你的 .ssh 文件夹中）。
2. 在服务器和本地创建一个名为 config 的文件，其中包含以下内容。

```
    Host github.com
    	HostName github.com
    	User git
    	IdentityFile ~/.ssh/id_rsa
```

现在你应该可以从服务器和本地拉取和推送提交！

### 第 1 部分： CUDA 热身 1：SAXPY（5 分）

为了练习编写 CUDA 程序，您的热身任务是用 CUDA 重新实现作业 1 中的 SAXPY 函数。这部分作业的启动代码位于作业库的 `/saxpy` 目录中。你可以在 `/saxpy` 目录中调用 `make` 和 `./cudaSaxpy` 来构建和运行 saxpy CUDA 程序。

请在 `saxpy.cu` 中的 `saxpyCuda` 函数中完成 SAXPY 的实现。在执行计算之前，您需要分配设备全局内存数组，并将主机输入数组 `X`、`Y` 和`result`的内容复制到 CUDA 设备内存中。CUDA 计算完成后，必须将结果复制回主机内存。请参阅《程序员指南》（网络版）第 3.2.2 节中 `cudaMemcpy` 函数的定义，或查看作业启动代码中指出的有用教程。

作为实现的一部分，在 `saxpyCuda` 中的 CUDA 内核调用周围添加计时器。添加后，程序应为两次执行计时：

- 所提供的启动代码包含计时器，用于测量将数据复制到 GPU、运行内核以及将数据复制回 CPU 的**整个过程**。

- 您还应该插入仅测量内核运行时间的计时器。(它们不应包括 CPU 到 GPU 的数据传输时间或将结果从 GPU 传输回 CPU 的时间）。

**在后一种情况下添加定时代码时，您需要小心谨慎：** 根据定义，CUDA 内核在 GPU 上的执行与 CPU 上运行的主应用程序线程是异步的。例如，如果您编写了如下代码:

```c++
double startTime = CycleTimer::currentSeconds();
saxpy_kernel<<<blocks, threadsPerBlock>>>(N, alpha, device_x, device_y, device_result);
double endTime = CycleTimer::currentSeconds();
```

你将测得内核执行时间似乎快得惊人！(因为你只是在计时 API 调用本身的成本，而不是在 GPU 上实际执行计算所产生的成本）。

因此，您需要在内核调用之后调用 `cudaDeviceSynchronize()`，以等待 GPU 上的所有 CUDA 工作完成。当 GPU 上所有先前的 CUDA 工作完成后，`cudaDeviceSynchronize()` 调用就会返回。请注意，`cudaDeviceSynchronize()` 并不是必须在 `cudaMemcpy()` 之后才能确保完成向 GPU 的内存传输，因为 `cudaMempy()` 在我们使用的条件下是同步的。(如需了解更多信息，请参阅[这个文档](https://docs.nvidia.com/cuda/cuda-runtime-api/api-sync-behavior.html#api-sync-behavior__memcpy-sync)）。

```c++
double startTime = CycleTimer::currentSeconds();
saxpy_kernel<<<blocks, threadsPerBlock>>>(N, alpha, device_x, device_y, device_result);
cudaDeviceSynchronize();
double endTime = CycleTimer::currentSeconds();
```

请注意，在您的测量中，包括向 CPU 传输数据和从 CPU 传输数据的时间，在最后计时器之前（在您调用 `cudaMemcopy()` 将数据返回 CPU 之后），没有必要调用 `cudaDeviceSynchronize()`，因为 `cudaMemcpy()` 在复制完成后才会返回调用线程。

**问题 1.** 与基于 CPU 的 SAXPY 顺序执行相比，您观察到的性能如何（请回顾作业 1 中程序 5 saxpy 的结果）？

**问题 2.** 比较并解释两组定时器提供的结果之间的差异（仅对内核执行进行定时与对将数据移动到 GPU 并返回内核执行的整个过程进行定时）。观察到的带宽值是否与报告的机器不同组件可用带宽基本一致？(您应该使用网络来追踪英伟达 T4 GPU 的内存带宽。提示：https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf。AWS 内存总线的预期带宽为 4 GB/s，与 16 通道 PCIe 3.0 的带宽不符。有几个因素会影响峰值带宽，包括 CPU 主板芯片组的性能，以及作为传输源的主机 CPU 内存是否 “针脚化”--后者允许 GPU 直接访问内存，而无需经过虚拟内存地址转换。如果您有兴趣，可以在这里找到更多信息：https://kth.instructure.com/courses/12406/pages/optimizing-host-device-data-communication-i-pinned-host-memory)

### 第 2 部分：CUDA 热身 2：并行前缀和（10 分）

现在，您已经熟悉了 CUDA 程序的基本结构和布局。作为第二个练习，要求您并行执行函数 `find_repeats`，即在给定整数 `A` 的情况下，返回 `A[i] == A[i+1]` 的所有索引 `i` 的列表。

例如，给定数组 `{1,2,2,1,1,1,3,5,3,3}`，程序应输出数组 `{1,3,4,8}`。

#### 排他前缀和

我们希望你在实现 `find_repeats` 时，首先实现并行的排他前缀相加操作。

排他性前缀相加取数组 `A`，并产生一个新的数组`output`，在每个索引 `i` 处都有截至但不包括 `A[i]` 的所有元素之和。例如，给定数组 `A={1,4,6,8,2}`，则排他前缀相加的输出结果为`output={0,1,5,11,19}`。

下面的 “类 C ”代码是扫描的迭代版本。在前面的伪代码中，我们使用 `parallel_for` 来表示潜在的并行循环。这也是我们在课堂上讨论过的算法： http://cs149.stanford.edu/fall23/lecture/dataparallel/slide_17

```c++
void exclusive_scan_iterative(int* start, int* end, int* output) {

    int N = end - start;
    memmove(output, start, N*sizeof(int));
    
    // upsweep phase
    for (int two_d = 1; two_d <= N/2; two_d*=2) {
        int two_dplus1 = 2*two_d;
        parallel_for (int i = 0; i < N; i += two_dplus1) {
            output[i+two_dplus1-1] += output[i+two_d-1];
        }
    }

    output[N-1] = 0;

    // downsweep phase
    for (int two_d = N/2; two_d >= 1; two_d /= 2) {
        int two_dplus1 = 2*two_d;
        parallel_for (int i = 0; i < N; i += two_dplus1) {
            int t = output[i+two_d-1];
            output[i+two_d-1] = output[i+two_dplus1-1];
            output[i+two_dplus1-1] += t;
        }
    }
}
```

我们希望您使用此算法在 CUDA 中实现并行前缀和的版本。您必须在 `scan/scan.cu` 中实现 `exclusive_scan` 函数。您的实现将由主机和设备代码组成。实现过程中需要启动多个 CUDA 内核（上述伪代码中的每个 parallel_for 循环启动一次）。

注意：在启动代码中，上述参考解决方案扫描实现假定输入数组的长度（`N`）是 2 的幂次。在 `cudaScan` 函数中，我们通过在 GPU 上分配相应缓冲区时将输入数组长度舍入到下一个 2 的幂次来解决这个问题。不过，代码只会将 `N` 个元素从 GPU 缓冲区复制回 CPU 缓冲区。这一事实可以简化 CUDA 的实现。

编译生成二进制 `cudaScan`。命令行用法如下:

```shell
Usage: ./cudaScan [options] 

Program Options:
  -m  --test <TYPE>      Run specified function on input.  Valid tests are: scan, find_repeats (default: scan)
  -i  --input <NAME>     Run test on given input type. Valid inputs are: ones, random (default: random)
  -n  --arraysize <INT>  Number of elements in arrays
  -t  --thrust           Use Thrust library implementation
  -?  --help             This message
```

#### 使用前缀和实现 “查找重复 ”功能

编写了 `exclusive_scan` 后，请在 `scan/scan.cu` 中实现 `find_repeats` 函数。除了调用一次或多次 `exclusive_scan()`，这还需要编写更多的设备代码。您的代码应将重复元素列表写入提供的输出指针（设备内存中），然后返回输出列表的大小。

调用 `exclusive_scan` 实现时，请记住`start`数组的内容会被复制到`output`数组。此外，传递给 `exclusive_scan` 的数组被假定在`device`内存中。

**评分**： 我们将测试你的代码在随机输入数组上的正确性和性能。

下面提供的扫描评分表显示了 K80 GPU 上简单 CUDA 实现的性能，以供参考。要检查`scan`和` find_repeats` 实现的正确性和性能得分，请分别运行 `./checker.pl scan` 和 `./checker.pl find_repeats`。这样做将生成如下所示的参考表；您的得分完全基于代码的性能。为获得满分，您的代码性能必须在所提供参考解决方案的 20% 以内。

```shell
-------------------------
Scan Score Table:
-------------------------
-------------------------------------------------------------------------
| Element Count   | Ref Time        | Student Time    | Score           |
-------------------------------------------------------------------------
| 1000000         | 0.766           | 0.143 (F)       | 0               |
| 10000000        | 8.876           | 0.165 (F)       | 0               |
| 20000000        | 17.537          | 0.157 (F)       | 0               |
| 40000000        | 34.754          | 0.139 (F)       | 0               |
-------------------------------------------------------------------------
|                                   | Total score:    | 0/5             |
-------------------------------------------------------------------------
```

这部分作业主要是让学生更多地练习编写 CUDA 和以数据并行的方式进行思考，而不是对代码进行性能调整。要想在这部分作业中获得满分，不需要做太多（或任何）性能调整，只需将算法伪代码直接移植到 CUDA 即可。不过，有一个小窍门：扫描的天真实现可能会为伪代码中并行循环的每次迭代启动 N 个 CUDA 线程，并使用内核中的条件执行来确定哪些线程实际需要工作。这样的解决方案性能不佳！（考虑到上扫阶段的最后一次最外层循环迭代，只有两个线程会执行工作！）。完全信用解决方案只会在最内层并行循环的每次迭代中启动一个 CUDA 线程。

**测试线束**： 默认情况下，测试线束运行在一个伪随机生成的数组上，该数组在每次运行程序时都是相同的，以帮助调试。你可以通过参数 `-i random` 在随机数组上运行，我们会在分级时这样做。我们鼓励你为程序设计其他输入，以帮助你评估程序。你还可以使用 `-n <size>` 选项来改变输入数组的长度。

参数 `--thrust` 将使用 Thrust 库的排他扫描实现。**如果有人能创造出与 Thrust 相媲美的实现方法，最多可获得两分加分。**

### 第 3 部分：简单的圆形渲染器（85 分）

现在开始真正的表演！

作业启动器代码的 `/render` 目录包含一个绘制彩色圆圈的渲染器实现。编译代码并使用以下命令行运行渲染器：`./render -r cpuref rgb`。程序将输出包含三个圆形的图像 `output_0000.ppm`。现在使用 `./render -r cpuref snow` 命令行运行渲染器。现在输出的图像将是飘落的雪花。在 OSX 上可以通过预览直接查看 PPM 图像。对于 Windows 操作系统，您可能需要下载一个查看器。

注意：也可以使用 `-i` 选项将渲染器输出发送到显示器，而不是文件。(在雪的情况下，你会看到雪花飘落的动画。）不过，要使用交互模式，你需要在本地机器上设置 X-windows 转发。(本参考资料或本参考资料可能会有所帮助）。

作业启动代码包含两个版本的渲染器：一个是顺序、单线程的 C++ 参考实现（在 `refRenderer.cpp` 中实现），另一个是不正确的并行 CUDA 实现（在 `cudaRenderer.cu` 中实现）。

#### 渲染器概述

我们建议您通过查看 `refRenderer.cpp` 中的参考实现来熟悉渲染器代码库的结构。在渲染第一帧之前，会调用 `setup` 方法。在使用 CUDA 加速的渲染器中，该方法可能包含所有渲染器初始化代码（分配缓冲区等）。每个帧都会调用 `render`，负责将所有圆绘制到输出图像中。呈现器的另一个主要函数 `advanceAnimation` 也是每帧调用一次。它会更新圆的位置和速度。在本任务中无需修改 `advanceAnimation`。

渲染器接受一个圆数组（三维位置、速度、半径、颜色）作为输入。渲染每帧的基本顺序算法是:

```c++
Clear image
for each circle
    update position and velocity
for each circle
    compute screen bounding box
    for all pixels in bounding box
        compute pixel center point
        if center point is within the circle
            compute color of circle at point
            blend contribution of circle into image for this pixel
```

下图展示了使用圆中点测试计算圆-像素覆盖率的基本算法。请注意，只有当像素的中心位于圆内时，圆才会为输出像素提供颜色。

![Point in circle test](handout/point_in_circle.jpg?raw=true "A simple algorithm for computing the contribution of a circle to the output image: All pixels within the circle's bounding box are tested for coverage. For each pixel in the bounding box, the pixel is considered to be covered by the circle if its center point (black dots) is contained within the circle. Pixel centers that are inside the circle are colored red. The circle's contribution to the image will be computed only for covered pixels.")

渲染器的一个重要细节是，它渲染的是半透明圆。因此，任何一个像素的颜色都不是单个圆的颜色，而是将重叠在该像素上的所有半透明圆的颜色混合在一起的结果（注意上面伪代码中的 “混合贡献 ”部分）。渲染器通过红色 (R)、绿色 (G)、蓝色 (B) 和不透明度 (alpha) 值 (RGBA) 的 4 元组来表示一个圆的颜色。Alpha = 1 表示圆完全不透明。Alpha = 0 对应完全透明的圆。要在一个颜色为 `P_r、P_g、P_b` 的像素上绘制一个颜色为 `C_r、C_g、C_b、C_alpha` 的半透明圆，渲染器会使用以下数学运算：

```c++
   result_r = C_alpha * C_r + (1.0 - C_alpha) * P_r
   result_g = C_alpha * C_g + (1.0 - C_alpha) * P_g
   result_b = C_alpha * C_b + (1.0 - C_alpha) * P_b
```

请注意，合成并不是交换的（对象 X 覆盖 Y 与对象 Y 覆盖 X 看起来并不相同），因此重要的是，渲染绘制圆圈的方式要遵循应用程序提供的顺序（可以假设应用程序提供的圆圈是按深度顺序绘制的）。(可以假设应用程序是按深度顺序提供圆的）例如，请看下面两张图片，其中蓝色圆画在绿色圆上，绿色圆画在红色圆上。在左边的图像中，圆是按照正确的顺序绘制到输出图像中的。在右图中，圆的绘制顺序不同，输出图像看起来也不正确。

![Ordering](handout/order.jpg?raw=true "The renderer must be careful to generate output that is the same as what is generated when sequentially drawing all circles in the order provided by the application.")

#### CUDA 渲染器

在熟悉了参考代码中实现的圆形渲染算法后，现在研究一下 `cudaRenderer.cu` 中提供的渲染器的 CUDA 实现。你可以使用 `--renderer cuda（或 -r cuda）`cuda 程序选项运行渲染器的 CUDA 实现。

所提供的 CUDA 实现会对所有输入圆进行并行计算，为每个 CUDA 线程分配一个圆。虽然该 CUDA 实现是圆渲染器数学的完整实现，但它包含几个主要错误，您将在本作业中修复这些错误。具体来说：当前的实现没有确保图像更新是原子操作，也没有保留所需的图像更新顺序（下文将介绍顺序要求）。

#### 渲染器要求

您的并行 CUDA 渲染器实现必须维护两个不变式，这两个不变式在顺序实现中是微不足道的。

1. **原子性：** 所有图像更新操作都必须是原子操作。关键区域包括读取四个 32 位浮点数值（像素的 rgba 颜色），将当前圆的贡献值与当前图像值混合，然后将像素的颜色写回内存。
2. **顺序：** 渲染器必须按照圆输入顺序对图像像素执行更新。也就是说，如果圆圈 1 和圆圈 2 都对像素 P 有贡献，则圆圈 1 对像素 P 的任何图像更新都必须在圆圈 2 对像素 P 更新之前应用到图像中。如上所述，保留排序要求可以正确呈现透明圆。(它对图形系统还有很多其他好处。如果好奇，请与 Kayvon 讨论）。**一个关键的观察点是，顺序的定义只规定了同一像素的更新顺序。**因此，如下图所示，对同一像素没有贡献的圆之间没有排序要求。这些圆可以独立处理。

![Dependencies](handout/dependencies.jpg?raw=true "The contributions of circles 1, 2, and 3 must be applied to overlapped pixels in the order the circles are provided to the renderer.")

由于所提供的 CUDA 实现无法满足这两项要求，因此在 rgb 和圆圈场景上运行 CUDA 渲染器实现，可以看到不正确尊重顺序或原子性的结果。如下图所示，您将看到水平条纹穿过生成的图像。这些条纹会随着每帧图像的变化而变化。

![Order_errors](handout/bug_example.jpg?raw=true "Errors in the output due to lack of atomicity in frame-buffer update (notice streaks in bottom of image).")

#### 你需要做什么

**您的任务是编写最快、最正确的 CUDA 渲染器实现。**您可以采取任何您认为合适的方法，但您的呈现器必须符合上述原子性和顺序要求。不符合这两项要求的解决方案在作业第 3 部分的得分将不超过 12 分。我们已经给出了这样的解决方案！

阅读 `cudaRenderer.cu` 并让自己相信它不符合正确性要求是一个好的开始。尤其要看看 `CudaRenderer:render` 是如何启动 CUDA 内核 `kernelRenderCircles` 的。(`kernelRenderCircles` 是所有工作发生的地方.）要直观地看到违反上述两个要求的效果，请使用 `make` 编译程序。然后运行 `./render -r cuda rand10k`，就会显示上图最下面一行所示的带有 10K 个圆的图像。将此图像（不正确）与运行 `./render -r cpuref rand10k` 运行顺序代码生成的图像进行比较。

我们建议您:

1. 首先重写 CUDA 启动代码实现，使其在并行运行时逻辑正确（我们建议采用不需要锁或同步的方法）

2. 然后确定您的解决方案存在哪些性能问题。

3. 此时，真正的作业思考开始了......（提示：`circleBoxTest.cu_inl` 中提供的 circle-intersects-box 测试是您的朋友。我们鼓励您使用这些子程序）。

以下是 `./render` 的命令行选项：

```shell
Usage: ./render [options] scenename
Valid scenenames are: rgb, rgby, rand10k, rand100k, biglittle, littlebig, pattern,
                      bouncingballs, fireworks, hypnosis, snow, snowsingle
Program Options:
  -r  --renderer <cpuref/cuda>  Select renderer: ref or cuda (default=cuda)
  -s  --size  <INT>             Make rendered image <INT>x<INT> pixels (default=1024)
  -b  --bench <START:END>       Run for frames [START,END)   (default [0,1))
  -f  --file  <FILENAME>        Output file name (FILENAME_xxxx.ppm)
  -c  --check                   Check correctness of CUDA output against CPU reference
  -i  --interactive             Render output to interactive display
  -?  --help                    This message
```

**检查代码 :**为了检测程序的正确性，`render` 提供了一个方便的 `--check` 选项。该选项会同时运行 CPU 参考渲染器和 CUDA 渲染器的顺序版本，然后比较生成的图像以确保正确性。您的 CUDA 渲染器所耗费的时间也会被打印出来。

我们总共提供了五个圆圈数据集，您将根据这些数据集进行评分。但是，为了获得满分，您的代码必须通过我们所有的正确性测试。要检查代码的正确性和性能得分，请运行 `/render` 目录中的 `./checker.py`（注意扩展名为 .py）。如果在起始代码上运行，程序将打印如下表格，以及整个测试集的结果：

```shell
------------
Score table:
------------
--------------------------------------------------------------------------
| Scene Name      | Ref Time (T_ref) | Your Time (T)   | Score           |
--------------------------------------------------------------------------
| rgb             | 0.2321           | (F)             | 0               |
| rand10k         | 5.7317           | (F)             | 0               |
| rand100k        | 25.8878          | (F)             | 0               |
| pattern         | 0.7165           | (F)             | 0               |
| snowsingle      | 38.5302          | (F)             | 0               |
| biglittle       | 14.9562          | (F)             | 0               |
--------------------------------------------------------------------------
|                                    | Total score:    | 0/72            |
--------------------------------------------------------------------------
```

注意：在某些运行中，您可能会获得其中一些场景的功劳，因为所提供的渲染器运行时间是非确定的，有时可能是正确的。但这并不能改变当前 CUDA 渲染器总体上不正确的事实。

“参考时间 ”是我们的参考解决方案在您当前机器上的性能（在提供的 `render_ref` 可执行文件中）。“您的时间 ”是您当前 CUDA 渲染器解决方案的性能，其中 `(F)` 表示解决方案不正确。您的成绩将取决于您的实现与这些参考实现相比的性能（请参阅《评分指南》）。

在提交代码的同时，我们还希望您提交一份清晰、高水平的实施方案工作原理说明，并简要介绍您是如何实现该解决方案的。请具体说明您一路尝试的方法，以及您如何确定如何优化您的代码（例如，您进行了哪些测量来指导您的优化工作？）

在撰写时应提及的工作内容包括:

1. 在文章顶部注明双方的姓名和 SUNet ID。
2. 复制为您的解决方案生成的分数表，并说明您在哪台机器上运行了您的代码。
3. 描述您是如何分解问题的，以及如何将工作分配给 CUDA 线程块和线程（甚至翘曲）的。
4. 描述解决方案中同步发生的位置。
5. 如果有的话，您采取了哪些措施来减少通信需求（例如同步或主存储器带宽需求）？
6. 简要描述你是如何得出最终解决方案的。一路上还尝试了哪些其他方法？它们有什么问题？

#### 评分指南

- 作业撰写占 7 分。
- 你的实施占 72 分。每个场景平均分为 12 分，具体如下：

  - 每个场景 2 分。
- 每个场景 10 分表现分（只有在解决方案正确的情况下才能获得）。您的表现将根据所提供的基准参考渲染器$T_{ref}$ 的表现进行评分：
  
  - 如果解决方案的时间 (T) 是$T_{ref}$的 10 倍，则不会获得性能分。
  
  - 对于优化方案的 20% 以内的解决方案（T < 1.20 * $T_{ref}$），将获得满分。
  
  - 对于其他 T 值（1.20 $T_{ref}$ <= T < 10 * $T_{ref}$），您的绩效得分将按 1 到 10 的比例计算： 10 * $T_{ref}$ / T。

- 您在班级排行榜上的实施表现将获得最后 6 分。有关排行榜的提交和评分细节将在随后的 Ed 帖子中详细说明。

- 如果解决方案的性能明显高于要求，最多可获得 5 分加分（由教师酌情决定）。您的书面材料必须清楚透彻地解释您的方法。

- 如果高质量的纯 CPU 并行渲染器实现能够很好地利用所有内核和内核的 SIMD 向量单元，最多可获得 5 分加分（由指导教师酌情决定）。请随意使用您所掌握的任何工具（如 SIMD 内核、ISPC、pthreads）。为获得学分，您应分析 GPU 解决方案和基于 CPU 解决方案的性能，并讨论实施选择不同的原因。

因此，本项目的总分如下：

- 第 1 部分（5 分）
- 第 2 部分（10 分）
- 第 3 部分撰写（7 分）
- 第 3 部分实施（72 分）
- 第三部分排行榜（6 分）
- 潜在加分（最多 10 分）

### 作业提示和技巧

以下是从前几年汇编的一组提示和技巧。请注意，有多种方法可以实现渲染器，并非所有提示都适用于您的方法。

- 此作业中有两个潜在的并行轴。一个轴是跨像素的并行性，另一个轴是跨圆的并行性（前提是重叠圆的排序要求得到尊重）。解决方案将需要利用这两种类型的并行性，可能在计算的不同部分.
- `circleBoxTest.cu_inl` 中提供给您的圆相交框测试是您的好朋友。我们鼓励您使用这些子例程。
- `exclusiveScan.cu_inl` 中提供的共享内存前缀和操作可能对您在这项作业中很有用（并非所有解决方案都可能选择使用它）。请参阅此处的前缀和的简单描述。我们提供了在共享内存中对 2 的幂大小数组的独占前缀和的实现。提供的代码不适用于非 2 的幂输入，并且还要求线程块中的线程数为数组的大小。请阅读代码中的注释。
- 如果您愿意，您可以在实现中使用 Thrust 库。Thrust 不是实现优化 CUDA 参考实现性能所必需的。有一种流行的解决问题的方法是使用我们提供的共享内存前缀和实现。还有另一种流行的方法是使用 Thrust 库中的前缀和例程。两者都是有效的解决方案策略。
- 渲染器中是否存在数据重用？可以做些什么来利用这种重用？
- 由于没有 CUDA 语言原语可以原子地执行图像更新操作的逻辑，您将如何确保图像更新的原子性？构建全局内存原子操作的锁定是一种解决方案，但请记住，即使您的图像更新是原子的，也必须按要求的顺序执行更新。我们建议您首先考虑确保并行解决方案中的顺序，然后再考虑解决方案中的原子性问题（如果它仍然存在）。
- 如果您有空闲时间，那就尽情制作自己的场景吧！

#### 捕捉 CUDA 错误

默认情况下，如果您访问的数组越界、分配过多内存或其他原因导致错误，CUDA 通常不会通知您；相反，它只会无声地失败并返回错误代码。您可以使用以下宏（可随意修改）来封装 CUDA 调用：

```c++
#define DEBUG

#ifdef DEBUG
#define cudaCheckError(ans) { cudaAssert((ans), __FILE__, __LINE__); }
inline void cudaAssert(cudaError_t code, const char *file, int line, bool abort=true)
{
   if (code != cudaSuccess) 
   {
      fprintf(stderr, "CUDA Error: %s at %s:%d\n", 
        cudaGetErrorString(code), file, line);
      if (abort) exit(code);
   }
}
#else
#define cudaCheckError(ans) ans
#endif
```

请注意，一旦代码正确无误，您可以取消定义 DEBUG 以禁用错误检查，从而提高性能。

然后，您就可以对 CUDA API 调用进行封装，以处理其返回的错误：

```c++
cudaCheckError( cudaMalloc(&a, size*sizeof(int)) );
```

请注意，您不能直接封装内核启动。相反，它们的错误会在你封装的下一个 CUDA 调用中被捕获：

```c++
kernel<<<1,1>>>(a); // suppose kernel causes an error!
cudaCheckError( cudaDeviceSynchronize() ); // error is printed on this line
```

所有 CUDA API 函数、`cudaDeviceSynchronize`、`cudaMemcpy`、`cudaMemset` 等都可以被封装。

**重要提示：**如果某个 CUDA 函数之前出错但未被捕获，则该错误将显示在下一次错误检查中，即使该函数封装了另一个函数。例如

```c++
...
line 742: cudaMalloc(&a, -1); // executes, then continues
line 743: cudaCheckError(cudaMemcpy(a,b)); // prints "CUDA Error: out of memory at cudaRenderer.cu:743"
...
```

因此，在调试时，建议您封装**所有** CUDA API 调用（至少在您编写的代码中）。

(出处：改编自 Stack Overflow 的[这篇帖子](https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api)）

### 3.4 上交说明

请使用 Gradescope 提交您的作业。如果您是与同伴合作，请记得在 Gradescope 上标记您的同伴。

1. 请以 `writeup.pdf` 文件格式提交作业。

2. 提交时请运行 `sh create_submission.sh`，生成压缩包并提交到 gradescope。请注意，这会在您的代码目录中运行 make clean，因此您必须再次运行 make 才能运行您的代码。如果脚本出错提示 “权限被拒绝”，您应该运行 `chmod +x create\_submission.sh` 并尝试重新运行脚本。

我们的评分脚本将重新运行校验代码，以验证您的分数是否与您在 `writeup.pdf` 中提交的分数一致。我们还可能尝试在其他数据集上运行您的代码，以进一步检查其正确性。

## 二、实验报告

### 1.SAXPY

#### （1）代码实现

```c++
//	
// CS149 TODO: allocate device memory buffers on the GPU using cudaMalloc.
//
cudaMalloc(&device_x, N * sizeof(float));
cudaMalloc(&device_y, N * sizeof(float));
cudaMalloc(&device_result, N * sizeof(float));
```

使用`cudaMalloc`函数在GPU上分配设备内存缓冲区。

```c++
//
// CS149 TODO: copy input arrays to the GPU using cudaMemcpy
//
cudaMemcpy(device_x, xarray, N * sizeof(float), cudaMemcpyHostToDevice);
cudaMemcpy(device_y, yarray, N * sizeof(float), cudaMemcpyHostToDevice);
```

使用`cudaMemcpy`函数将数据从主机（CPU）内存复制到设备（GPU）内存。

```c++
//
// CS149 TODO: copy result from GPU back to CPU using cudaMemcpy
//
cudaMemcpy(resultarray, device_result, N * sizeof(float), cudaMemcpyDeviceToHost);
```

使用`cudaMemcpy`函数将数据从设备（GPU）内存复制到主机（CPU）内存。

```c++
//
// CS149 TODO: free memory buffers on the GPU using cudaFree
//
cudaFree(device_x);
cudaFree(device_y);
cudaFree(device_result);
```

使用`cudaFree`函数释放GPU上分配的内存缓冲区。

#### （2）时间测量

使用如下代码完成对GPU上CUDA工作时间的测量计算：

```c++
double startTime_kernel = CycleTimer::currentSeconds();
saxpy_kernel<<<blocks, threadsPerBlock>>>(N, alpha, device_x, device_y, device_result);
cudaDeviceSynchronize();
double endTime_kernel = CycleTimer::currentSeconds();
```

运行程序，得到如下结果：

```shell
$ ./cudaSaxpy
---------------------------------------------------------
Found 1 CUDA devices
Device 0: NVIDIA GeForce RTX 3070 Ti
   SMs:        48
   Global mem: 8192 MB
   CUDA Cap:   8.6
---------------------------------------------------------
Running 3 timing tests:
Effective BW by CUDA saxpy: 175.799 ms          [6.357 GB/s]
         Kernel time: 9.098 ms
Effective BW by CUDA saxpy: 164.700 ms          [6.786 GB/s]
         Kernel time: 2.328 ms
Effective BW by CUDA saxpy: 171.018 ms          [6.535 GB/s]
         Kernel time: 2.406 ms
```

#### （3）问题思考

##### 与基于 CPU 的顺序 SAXPY 实现相比，你观察到的性能如何？

观察到CUDA 实现的 SAXPY 内核函数执行速度远快于基于 CPU 的顺序实现。

##### 比较并解释两组计时器提供的结果之间的差异。

**测试1:**

- **总时间（包括数据传输）**: 175.799 ms
- **内核时间（仅计时内核执行）**: 9.098 ms

**测试2:**

- **总时间（包括数据传输）**: 164.700 ms
- **内核时间（仅计时内核执行）**: 2.328 ms

**测试3:**

- **总时间（包括数据传输）**: 171.018 ms
- **内核时间（仅计时内核执行）**: 2.406 ms

##### 差异分析

**1. 数据传输时间的影响**

从上述数据中可以看出，总时间远大于内核时间。这是因为总时间包括了以下步骤：

- 将数据从主机（CPU）传输到设备（GPU）。
- 执行内核函数。
- 将结果从设备（GPU）传输回主机（CPU）。

这些传输步骤可能会占用大量时间，尤其是对于小数据集或当数据传输效率较低时。

**2. 内核执行时间**

内核时间（仅计时内核执行）明显少于总时间。这说明 GPU 执行内核计算的速度非常快，得益于 GPU 的并行计算能力。实际的计算部分并不是性能的瓶颈。

### 2.Parallel Prefix-Sum

#### （1）代码实现

首先实现**数组的并行排他前缀和运算**，代码如下：

```c++
__global__ void upsweep_kernel(const int two_d, const int two_dplus1,  int *output, const int N)
{
    unsigned int idx{blockIdx.x * blockDim.x + threadIdx.x};
    unsigned int k{two_dplus1 * idx};
    if (k < N)
    {
        output[k + two_dplus1 - 1] = output[k + two_d - 1] + output[k + two_dplus1 - 1];
    }
}

__global__ void downsweep_kernel(const int two_d, const int two_dplus1, int *output, const int N)
{
    unsigned int idx{blockIdx.x * blockDim.x + threadIdx.x};
    unsigned int k{two_dplus1 * idx};
    if (k < N)
    {
        int tmp{output[k + two_d - 1]};
        output[k + two_d - 1] = output[k + two_dplus1 - 1];
        output[k + two_dplus1 - 1] = tmp + output[k + two_dplus1 - 1];
    }
}

void exclusive_scan(int *input, int N, int *result)
{

    // CS149 TODO:
    //
    // Implement your exclusive scan implementation here.  Keep input
    // mind that although the arguments to this function are device
    // allocated arrays, this is a function that is running in a thread
    // on the CPU.  Your implementation will need to make multiple calls
    // to CUDA kernel functions (that you must write) to implement the
    // scan.

    // input and result are on device
    cudaMemcpy(result, input, N * sizeof(int), cudaMemcpyDeviceToDevice);

    // Up Sweep phase
    int rounded_length = nextPow2(N);
    for (int two_d = 1; two_d < rounded_length / 2; two_d <<= 1)
    {
        int two_dplus1 = (two_d << 1);
        int threads_per_block{std::min(rounded_length / two_dplus1, THREADS_PER_BLOCK)};
        int num_blocks{(rounded_length / two_dplus1 + threads_per_block - 1) / threads_per_block};
        upsweep_kernel<<<num_blocks, threads_per_block>>>(two_d, two_dplus1,  result, rounded_length);
        cudaDeviceSynchronize();
    }
    // Down Sweep pahse
    cudaMemset(&result[rounded_length - 1], 0, sizeof(int));
    for (int two_d = rounded_length / 2; two_d >= 1; two_d >>= 1)
    {
        int two_dplus1 = (two_d << 1);
        int threads_per_block{std::min(rounded_length / two_dplus1, THREADS_PER_BLOCK)};
        int num_blocks{(rounded_length / two_dplus1 + threads_per_block - 1) / threads_per_block};
        downsweep_kernel<<<num_blocks, threads_per_block>>>(two_d, two_dplus1, result, rounded_length);
        cudaDeviceSynchronize();
    }
}
```

##### 前缀和计算核函数

1.Upsweep Kernel:

   这个 kernel 实现了 upsweep 阶段的一步。`two_d` 和 `two_dplus1` 是当前步的两个重要参数。每个线程处理一个 `k` 值，`k` 由 `two_dplus1` 和 `idx` 确定。它更新 `output` 数组，使其包含从 `k + two_d - 1` 和 `k + two_dplus1 - 1` 的和。

2.Downsweep Kernel:

这个 kernel 实现了 downsweep 阶段的一步。类似于 upsweep，每个线程处理一个 `k` 值。这里更新 `output` 数组，使其包含从 `k + two_d - 1` 到 `k + two_dplus1 - 1` 的值，并在 `output` 中交换这些值。

##### Exclusive Scan 主函数

通过 upsweep 和 downsweep 两个阶段来完成前缀和的计算。

执行`cudaScan`函数结果如下：

```shell
$ ./cudaScan -m scan
---------------------------------------------------------
Found 1 CUDA devices
Device 0: NVIDIA GeForce RTX 3070 Ti
   SMs:        48
   Global mem: 8192 MB
   CUDA Cap:   8.6
---------------------------------------------------------
Array size: 64
Student GPU time: 0.465 ms
Scan outputs are correct!
```

然后我们**使用前缀和实现“查找重复”**，代码如下：

```c++
__global__ void repetition_kernel(int* input, int* indicator, const int N) {
    unsigned int idx {blockIdx.x * blockDim.x + threadIdx.x};
    if (idx < N - 1 ) {
        indicator[idx] = input[idx] == input[idx + 1]?1:0;
    }
}

__global__ void repeats_out(int* prefixSum, int* indicator, int* output) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (indicator[i]==1) {
        output[prefixSum[i]] = i;
    }
}

int find_repeats(int* device_input, int length, int* device_output) {

    // CS149 TODO:
    //
    // Implement this function. You will probably want to
    // make use of one or more calls to exclusive_scan(), as well as
    // additional CUDA kernel launches.
    //    
    // Note: As in the scan code, the calling code ensures that
    // allocated arrays are a power of 2 in size, so you can use your
    // exclusive_scan function with them. However, your implementation
    // must ensure that the results of find_repeats are correct given
    // the actual array length.
    const int rounded_length {nextPow2(length)};
    
    int threads_per_block {std::min(rounded_length, THREADS_PER_BLOCK)};
    int num_blocks {(rounded_length + THREADS_PER_BLOCK - 1) / threads_per_block};
    
    // turn device_output into indicator
    repetition_kernel<<<num_blocks, threads_per_block>>>(device_input, device_output, length);

    // turn device_input into prefixSum
    exclusive_scan(device_output, length, device_input);

    repeats_out<<<num_blocks,threads_per_block>>>(device_input,device_output,device_output);

    int output_length;
    cudaMemcpy(&output_length, device_input+length-1, sizeof(int), cudaMemcpyDeviceToHost);
    
    return output_length; 
}
```

##### 查找重复核函数

1.repetition_kernel：

这个kernal用于生成一个指示器数组，指示`device_input` 中哪些元素与其相邻元素相同，将这个`indicator`数组的结果存到`device_output`数组中。

2.repeats_out：

生成最终输出数组。

##### find_repeats 主函数

首先使用`repetition_kernel` 内核生成 `device_output` 数组，指示哪些元素与其相邻元素相同。

然后使用`exclusive_scan` 计算指示器数组的前缀和，结果存储在 `device_input` 数组中。

最后将`device_input`作为`prefixSum`数组，将`device_output`作为`indicator`数组进入`repeats_out`内核，使用前缀和数组和指示器数组生成最终的输出数组 `device_output`。

在输出中，从前缀和数组`device_input`的最后一个元素中获取重复元素的数量，即输出数组的长度。

执行`cudaScan`函数结果如下：

```shell
$ ./cudaScan -m find_repeats
---------------------------------------------------------
Found 1 CUDA devices
Device 0: NVIDIA GeForce RTX 3070 Ti
   SMs:        48
   Global mem: 8192 MB
   CUDA Cap:   8.6
---------------------------------------------------------
Array size: 64
Student GPU time: 0.541 ms
Find_repeats outputs are correct!
```

#### （2）运行测试

运行**`./checker.pl scan`**，结果如下：

```shell
$ ./checker.pl scan
Test: scan
--------------
Running tests:
--------------

Element Count: 1000000
Correctness passed!
Student Time: 2.891
Ref Time: 6.763

Element Count: 10000000
Correctness passed!
Student Time: 5.153
Ref Time: 8.664

Element Count: 20000000
Correctness passed!
Student Time: 8.489
Ref Time: 12.822

Element Count: 40000000
Correctness passed!
Student Time: 16.363
Ref Time: 22.908

-------------------------
Scan Score Table:
-------------------------
-------------------------------------------------------------------------
| Element Count   | Ref Time        | Student Time    | Score           |
-------------------------------------------------------------------------
| 1000000         | 6.763           | 2.891           | 1.25            |
| 10000000        | 8.664           | 5.153           | 1.25            |
| 20000000        | 12.822          | 8.489           | 1.25            |
| 40000000        | 22.908          | 16.363          | 1.25            |
-------------------------------------------------------------------------
|                                   | Total score:    | 5/5             |
-------------------------------------------------------------------------
```

运行**`./checker.pl find_repeats`**，结果如下：

```shell
$ ./checker.pl find_repeats
Test: find_repeats
--------------
Running tests:
--------------

Element Count: 1000000
Correctness passed!
Student Time: 2.136
Ref Time: 2.244

Element Count: 10000000
Correctness passed!
Student Time: 5.930
Ref Time: 12.025

Element Count: 20000000
Correctness passed!
Student Time: 9.123
Ref Time: 23.588

Element Count: 40000000
Correctness passed!
Student Time: 15.752
Ref Time: 44.519

-------------------------
Find_repeats Score Table:
-------------------------
-------------------------------------------------------------------------
| Element Count   | Ref Time        | Student Time    | Score           |
-------------------------------------------------------------------------
| 1000000         | 2.244           | 2.136           | 1.25            |
| 10000000        | 12.025          | 5.930           | 1.25            |
| 20000000        | 23.588          | 9.123           | 1.25            |
| 40000000        | 44.519          | 15.752          | 1.25            |
-------------------------------------------------------------------------
|                                   | Total score:    | 5/5             |
-------------------------------------------------------------------------
```

### 3.A Simple Circle Renderer 

#### （1）代码实现

这个 CUDA 内核 `kernelRenderCircles` 用于渲染一个图像，其中图像上的每个像素由多个圆（circle）进行着色。内核的主要工作是检查每个像素与哪些圆相交，并根据这些相交的圆对像素进行着色。以下是代码的详细解释：

##### 主要步骤

1. **初始化与线程信息**：
   
   ```cpp
   uint numCircles = cuConstRendererParams.numCircles;
   int imgWidth = cuConstRendererParams.imageWidth;
   int imgHeight = cuConstRendererParams.imageHeight;
   float invWidth = 1.f / imgWidth;
   float invHeight = 1.f / imgHeight;
   
   uint threadId = threadIdx.y * blockDim.x + threadIdx.x;
   uint pixelX = blockIdx.x * blockDim.x + threadIdx.x;
   uint pixelY = blockIdx.y * blockDim.y + threadIdx.y;
   ```
   
   - 获取图像的宽度、高度及其倒数。
   - 获取线程ID和像素位置。
   
2. **计算像素位置和局部图像数据**：
   ```cpp
   float4 *imgPtrGlobal = (float4 *)(&cuConstRendererParams.imageData[4 * (pixelY * imgWidth + pixelX)]);
   float4 imgPtrLocal = *imgPtrGlobal;
   float2 pixelCenter = make_float2(invWidth * (static_cast<float>(pixelX) + 0.5f),
                                    invHeight * (static_cast<float>(pixelY) + 0.5f));
   ```

   - 计算像素中心位置，并从全局内存中读取图像数据到局部变量 `imgPtrLocal`，以减少全局内存访问。

3. **计算像素块边界**：
   ```cpp
   float borderL = fminf(1.f, invWidth * (static_cast<float>(blockIdx.x * blockDim.x)));
   float borderR = fminf(1.f, invWidth * (static_cast<float>((blockIdx.x + 1) * blockDim.x) + 1.f));
   float borderB = fminf(1.f, invHeight * (static_cast<float>(blockIdx.y * blockDim.y)));
   float borderT = fminf(1.f, invHeight * (static_cast<float>((blockIdx.y + 1) * blockDim.y) + 1.f));
   ```

   - 计算当前像素块的边界，用于后续判断圆是否与该块相交。

4. **共享内存声明**：
   ```cpp
   __shared__ uint CircleCount;
   __shared__ uint circleIntersectBlock[SCAN_BLOCK_DIM];
   __shared__ float circleIntersectR[SCAN_BLOCK_DIM];
   __shared__ float3 circleIntersectC[SCAN_BLOCK_DIM];
   __shared__ uint prefixSum[2 * SCAN_BLOCK_DIM];
   uint *CircleIndex = prefixSum;
   ```

   - 声明用于存储圆与像素块相交信息的共享内存。

##### 圆渲染步骤

1. **数据定义**：

	```c++
	#define SCAN_BLOCK_DIM 512
	```

2. **遍历所有圆**：

   ```cpp
   for (uint index = 0; index < numCircles; index += SCAN_BLOCK_DIM)
   ```

   - 每次处理 `SCAN_BLOCK_DIM` 个圆。

3. **检查圆是否与像素块相交**：
   ```cpp
   circleIndex = index + threadId;
   bool lastIntersect = false;
   if (circleIndex < numCircles)
   {
       circleCenter = *(float3 *)(&cuConstRendererParams.position[3 * circleIndex]);
       circleR = cuConstRendererParams.radius[circleIndex];
       circleIntersectBlock[threadId] = circleInBoxConservative(circleCenter.x, circleCenter.y, circleR, borderL, borderR, borderT, borderB);
       if (threadId == SCAN_BLOCK_DIM - 1)
       {
           lastIntersect = (circleIntersectBlock[threadId] == 1);
       }
   }
   else
   {
       circleIntersectBlock[threadId] = 0;
   }
   
   if (threadId == 0)
   {
       CircleCount = 0;
   }
   __syncthreads();
   ```

   - 每个线程检查一个圆是否与像素块相交，并更新相交信息。
   - 使用共享内存 `circleIntersectBlock` 存储相交信息。

4. **前缀和计算**：
   ```cpp
   sharedMemExclusiveScan(threadId, circleIntersectBlock, circleIntersectBlock, prefixSum,
                          SCAN_BLOCK_DIM);
   __syncthreads();
   ```

   - 使用前缀和计算将相交信息转换为圆的索引。

5. **收集相交圆信息**：
   ```cpp
   if (lastIntersect ||
       (threadId < SCAN_BLOCK_DIM - 1 && circleIntersectBlock[threadId] != circleIntersectBlock[threadId + 1]))
   {
       int circleI = circleIntersectBlock[threadId];
       atomicAdd(&CircleCount, 1);
       CircleIndex[circleI] = circleIndex;
       circleIntersectR[circleI] = circleR;
       circleIntersectC[circleI] = circleCenter;
   }
   __syncthreads();
   ```

   - 根据前缀和结果，将相交圆的信息存储到共享内存中。

6. **对像素进行着色**：
   
   ```cpp
   for (int circleI = 0; circleI < CircleCount; circleI++)
   {
       shadePixel(CircleIndex[circleI], pixelCenter,
                  circleIntersectC[circleI], &imgPtrLocal, circleIntersectR[circleI]);
   }
   __syncthreads();
   ```
   
   - 使用相交圆的信息对当前像素进行着色。

##### 最后步骤

```cpp
*imgPtrGlobal = imgPtrLocal;
```

- 将局部存储的像素数据写回到全局内存。

运行`./render`结果如下：

```shell
$ ./render -r cuda -c rgb
Rendering to 1024x1024 image
Loaded scene with 3 circles
Loaded scene with 3 circles
---------------------------------------------------------
Initializing CUDA for CudaRenderer
Found 1 CUDA devices
Device 0: NVIDIA GeForce RTX 3070 Ti
   SMs:        48
   Global mem: 8192 MB
   CUDA Cap:   8.6
---------------------------------------------------------

Running benchmark, 1 frames, beginning at frame 0 ...
Dumping frames to output_xxx.ppm
Copying image data from device
Wrote image file output_0000.ppm
Copying image data from device
***************** Correctness check passed **************************
Clear:    0.2165 ms
Advance:  0.0007 ms
Render:   0.2517 ms
Total:    0.4689 ms
File IO:  137.5229 ms

Overall:  0.1546 sec (note units are seconds)
```

### （2）运行测试

运行**`./checker.py`**，结果如下：

```shell
$ ./checker.py

Running scene: rgb...
[rgb] Correctness passed!
[rgb] Student times:  [0.3427, 0.3894, 0.4217]
[rgb] Reference times:  [0.3186, 0.3164, 0.3016]

Running scene: rgby...
[rgby] Correctness passed!

Running scene: rand10k...
[rand10k] Correctness passed!
[rand10k] Student times:  [1.6665, 1.6886, 1.6609]
[rand10k] Reference times:  [1.8437, 1.8312, 1.8491]

Running scene: rand100k...
[rand100k] Correctness passed!
[rand100k] Student times:  [14.0522, 13.491, 15.1962]
[rand100k] Reference times:  [17.7013, 17.8352, 17.8661]

Running scene: biglittle...
[biglittle] Correctness passed!
[biglittle] Student times:  [10.8299, 10.6378, 11.2964]
[biglittle] Reference times:  [9.8504, 10.3943, 10.8515]

Running scene: littlebig...
[littlebig] Correctness passed!

Running scene: pattern...
[pattern] Correctness passed!
[pattern] Student times:  [0.368, 0.459, 0.3496]
[pattern] Reference times:  [0.3785, 0.382, 0.3871]

Running scene: bouncingballs...
[bouncingballs] Correctness passed!

Running scene: hypnosis...
[hypnosis] Correctness passed!

Running scene: fireworks...
[fireworks] Correctness passed!

Running scene: snow...
[snow] Correctness passed!

Running scene: snowsingle...
[snowsingle] Correctness passed!
[snowsingle] Student times:  [6.7751, 6.2545, 6.7578]
[snowsingle] Reference times:  [11.1163, 12.2165, 10.4082]
------------
Score table:
------------
--------------------------------------------------------------------------
| Scene Name      | Ref Time (T_ref) | Your Time (T)   | Score           |
--------------------------------------------------------------------------
| rgb             | 0.3016           | 0.3427          | 12              |
| rand10k         | 1.8312           | 1.6609          | 12              |
| rand100k        | 17.7013          | 13.491          | 12              |
| pattern         | 0.3785           | 0.3496          | 12              |
| snowsingle      | 10.4082          | 6.2545          | 12              |
| biglittle       | 9.8504           | 10.6378         | 12              |
--------------------------------------------------------------------------
|                                    | Total score:    | 72/72           |
--------------------------------------------------------------------------
```

## 三、总结

在这个 CUDA 实验中，主要涉及到了两个主要的任务：前缀和计算和图像渲染。下面是对这两个任务的总结：

### 前缀和计算（Exclusive Scan）

1. **任务描述**：
   - 实现了一个并行的前缀和计算算法，用于对数组进行累积求和操作。

2. **实现方式**：
   - 使用了 CUDA 的并行计算能力，利用了 GPU 的并行处理单元。
   - 使用了两个 CUDA kernel 函数：`upsweep_kernel` 和 `downsweep_kernel`，分别代表了上扫描和下扫描的过程。
   - 使用了共享内存来存储中间结果，以减少全局内存访问次数。
   - 实现了并行的扫描和归约算法，以提高计算效率。

3. **性能优化**：
   - 通过调整线程块大小、使用共享内存等方式优化了计算性能。
   - 使用了优化的扫描算法，如树形扫描算法，以减少计算时间。

4. **结果验证**：
   - 通过比较计算结果与预期结果来验证算法的正确性。
   - 使用了 CUDA 的设备端断言来检查计算过程中的错误。

### 图像渲染（Circle Rendering）

1. **任务描述**：
   - 实现了一个 CUDA 内核函数，用于将圆形渲染到图像上。

2. **实现方式**：
   - 使用了 CUDA 的并行计算能力，利用了 GPU 的并行处理单元。
   - 通过计算每个像素的位置和圆的边界，判断像素是否在圆内，从而确定是否需要渲染。
   - 使用了线程块和线程的索引来分配工作，以并行地渲染多个圆。

3. **性能优化**：
   - 通过合理设置线程块大小、优化数据访问模式等方式优化了渲染性能。
   - 尽可能减少了全局内存访问次数，使用了共享内存来存储中间结果。

4. **结果验证**：
   - 通过观察图像的渲染结果来验证算法的正确性。
   - 使用了 CUDA 的设备端断言来检查渲染过程中的错误。

总的来说，这个实验结合了 CUDA 编程的核心概念，如并行计算、线程模型、内存访问优化等，通过实现前缀和计算和图像渲染两个任务，帮助理解了如何在 CUDA 中利用 GPU 并行计算的能力来解决实际问题。
